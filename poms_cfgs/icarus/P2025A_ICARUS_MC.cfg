###############################################################################################################
#
#                                  ICARUS experiment config file [2025]
#
###############################################################################################################
#
# This file is intended to be the reference config file for the ICARUS experiment. 
# The config options are grouped into sections. In this file several sections are specified:
#
###############################################################################################################
#  - [global]
#    - includes = file1 file2 ... -- other config files to incude and merge with this one
#    - group = jobsub-group-name  -- group name for jobsub_submit, etc.
#    - xxx = yyy                  -- macro for replacement anywhere in file
#
###############################################################################################################
#  - [env_pass]
#    - specific environment variable and value to pass in with -e option to jobsub
#
###############################################################################################################
#  - [submit]: Any arguments to jobsub_submit (minus the dashes)
#    - group = groupname   -- for --group=groupname
#    - resource-provides = -- usually: usage_model=OFFSITE,OPPORTUNISTIC,DEDICATED
#    - site = sitelist     -- list sites to restrict to
#    - expected-lifetime = -- expected run time of job
#    - disk =              -- disk required
#    - memory =            -- available memory required
#    - role =              -- VOMS ROle for priorities/accounting
#    - N =                 -- number of jobs
#    - lines =             -- line to append to condor submit file
#    - dataset =           -- Submit SAM start/end project DAG on this dataset. Properly an alias for dataset_definition in version v3_3_3 and later.
#  Run "jobsub_submit --group=group --help" to get a full list. But also:
#    - n_files_per_job =   -- divide count of files in dataset by this to get -N value (new in v3_2_3)
#  You can append a flag-name with _0, _1, _2.. to specify multiple options of --lines or -f, etc. i.e.
#           [submit]
#           f_0 = file1
#           f_1 = file2
#           f_2 = file3
#
###############################################################################################################
#  - [job_setup]: These are all flags passed to the fife_wrap wrapper script in your job. 
#                 Anything listed in this section will be passed to fife_wrap as a flag whether 
#                 or not it is listed here.
#    - debug = true                 -- turn on debug messages in the fife_wrap script
#    - inputfile = path-to-file     -- file to copy into job
#    - inputtar = path-to-tarfile   -- tarfile to copy into job and unpack
#    - find_setups = true           -- look for and source "setups.sh" script in /cvmfs/fermilab.opensciencegrid.org/products/common, etc.
#    - export = name=value          -- environment variable to set
#    - setup = setup-args           -- ups setup command to run
#    - spack-load = spack-load-args -- do a "spack load $spec" to load a spack distributed package in your job. (v3_3 and later)
#    - setup_local = true           -- setup packages in INPUT_TAR are (see --tar_file_name in jobsub_submit to get it there)
#    - source = path-to-file        -- (bash) script to source
#    - prescript = bash-command     -- script to run before main executable (runs in a separate subshell but environment changes are collected)
#    - postscript = bash-command    -- script to run after main executable (runs in a separate subshell but environment changes are collected)
#    - mix = bash-command           -- script to run on input files (using ${fname} / ${furi} in the environment) before running first executable
#    - with_gdb = True              -- run executable(s) under gdb and print a stack trace
#  In addition you can specify one of the following flags that specify multiple-file/SAM consumer behavior
#    - ifdh_art = True              -- use an ifdh_art mainline -- start SAM consumer, executable will do getNextFile loop internally
#    - multifile = True             -- use the multifile mainline -- Start Sam consumer run getNextFile loop with executable run per file with filename on end of first executable line
#    - getconfig = true             -- like multifile but the file is given with -c filename as a config file
#  You can append a config item with _0, _1, _2.. to specify multiple options of --setup or --source, i.e.
#           [submit]
#           setup_1 = package2 -q quals
#           setup_2 = package3
#
###############################################################################################################
#  - [sam_consumer]: Specify options for the establishConsumer call if using 
#                    ifdh_art/multifile/getconfig options above
#    - limit = n         -- limit number of files to send this consumer
#    - schema = x        -- file delivery schema(s) preferred {root (streaming),gsiftp,..}
#    - appname = name    -- application name to associate with consumer
#    - appfamily = name  -- application family "
#    - appvers = version -- application version "
#
###############################################################################################################
#  - [executable]: Primary command your job is going to run. Allows you to specify further 
#                  executables to run -- i.e. if you want to run a simulation and reco in the same job
#    - name = executable-name  -- primary executable to run
#    - arg_1 = first-argument  -- first argument
#    - arg_2 = second-argument -- second argument
#    - exe_stdout = yyy        -- file to redirect standard output
#    - exe_stderr = yyy        -- file to redirect standard error
#
###############################################################################################################
#  - [job_output]
#    - outputlist = filename        -- "ifdh cp -f " file for copying back results
#    - addoutput = pattern          -- pattern to give to "ifdh addOutputFile"
#    - dest = path                  -- destination directory for copyBackOutput
#    - hash = n                     -- number of filename-hash subdirectories to make in dest
#    - rename = how                 -- argument for "ifdh renameOutput"
#    - declare_metadata = True      -- declare metadata for output files to SAM
#    - metadata_extractor = command -- add a metadata extractor command
#    - filter_metadata x,y,z        -- comma separated list of metadata fields to drop before declaring (i.e. to remove fields the metadata extractor found)
#    - add_metadata x=y             -- add additional metadata variables to declared metadata
#    - add_to_dataset name          -- add a metadata dataset.tag value to files, and declare as a dataset. A special shorthand "_poms_task" (or "_poms_analysis" for analysis users) can be given for the name, which will be replaced with poms_depends_${POMS_TASK_ID}_1 (${USER}_poms_depends_${POMS_TASK_ID} for analysis) for the input dataset for a dependant stage.
#    - dataset_exclude = glob       -- exclude files matching glob from dataset
#    - add_location = True          -- add a location for file after copying it to dest
#    - parallel = N                 -- [version v3_4_0 and later] run N copy threads in parallel when copying back data -- good for single jobs copying back lots of small files, NOT recommended for jobs with a large number of jobs submitted at once. .
#    - hash_alg = name              -- [version v3_4_0 and later] use particular hash for hash subdirectories. Default is "md5", can also specify "sha256".
#
###############################################################################################################
#  - [stage_xxx]: In the stage_xxx sections you can specify overrides to the other 
#                 parameters that will take effect if --stage xxx is passed on the command line, i.e.
#    - section.var = value
#  will override the value for var in section [section] For example, if you wanted to make a 
#  stage "middle" that was different in that it passed a different config file, you could have:
#           [executable]
#           exe = fred
#           arg_1 = -c
#           arg_2 = usual.cfg
#
#           [stage_middle]
#           executable.arg_2 = different.cfg
#
###############################################################################################################
#
# Below you can find a brief description of the purpouse and the usage of the commands: 
#  - JOBSUB_SUBMIT
#  - FIFE_LAUNCH
#  - FIFE_WRAP
#
###############################################################################################################
# ----------------------------------------------------------------------------
# JOBSUB_SUBMIT
# ----------------------------------------------------------------------------
# 
# jobsub_submit is the job submission command of jubsub, a batch submission system.
#
#    usage: jobsub_submit [-h] [--auth-methods AUTH_METHODS] [-G GROUP] [--global-pool GLOBAL_POOL] [--role ROLE] [--subgroup SUBGROUP]
#                         [--verbose VERBOSE] [--debug] [--devserver] [--version] [--support-email] [--job-info JOB_INFO]
#                         [--need-storage-modify NEED_STORAGE_MODIFY] [--need-scope NEED_SCOPE] [-c APPEND_CONDOR_REQUIREMENTS]
#                         [--blocklist BLOCKLIST] [-r R] [-i I] [-t T] [--cmtconfig CMTCONFIG] [--cpu NUMBER] [--dag]
#                         [--dataset-definition DATASET_DEFINITION] [--dd-percentage DD_PERCENTAGE] [--dd-extra-dataset DD_EXTRA_DATASET]
#                         [--disk DISK] [-d tag dir] [--email-to EMAIL_TO] [-e ENVIRONMENT] [--expected-lifetime EXPECTED_LIFETIME]
#                         [-f INPUT_FILE] [--generate-email-summary] [--gpu NUMBER] [-L LOG_FILE] [-l LINES] [--project-name PROJECT_NAME] [-Q]
#                         [--mail_on_error] [--mail_always] [--maxConcurrent MAXCONCURRENT] [--managed-token] [--memory MEMORY] [-N N] [-n]
#                         [--no-env-cleanup] [--OS OS] [--overwrite-condor-requirements OVERWRITE_CONDOR_REQUIREMENTS]
#                         [--resource-provides RESOURCE_PROVIDES] [--skip-check SKIP_CHECK] [--tar_file_name TAR_FILE_NAME]
#                         [--tarball-exclusion-file TARBALL_EXCLUSION_FILE] [--timeout TIMEOUT] [--use-cvmfs-dropbox] [--use-pnfs-dropbox]
#                         [--site SITE | --onsite | --offsite] [--singularity-image SINGULARITY_IMAGE | --no-singularity]
#                         [executable] ...
#
# Documentation: https://cdcvs.fnal.gov/redmine/projects/jobsub/wiki/Jobsub_submit
# 
# ----------------------------------------------------------------------------
# FIFE_LAUNCH
# ----------------------------------------------------------------------------
# 
# To handle the wide variety of command line options of jubsub_sbumit, fife_launch 
# was introduced. fife_launch accepts (and needs) a configuration file as input. 
# In the file the options can be specified in a more structurated manner.
#
#  Usage: fife_launch --config file.cfg --override foo.bar=10
#
# Documentation: https://cdcvs.fnal.gov/redmine/projects/fife_utils/wiki/Fife_launch_Reference
# 
# ----------------------------------------------------------------------------
# FIFE_WRAP
# ----------------------------------------------------------------------------
# 
# Script to wrap actual script to be run. It will setup the environment 
# variables and run the actual script.
# 
#  Usage: fife_wrap [options]
#  
#  Options:
#    -h, --help            show this help message and exit
#    -q QUALS, --quals=QUALS
#                          Set qualifiers for ifdh_art setup
#    -v VERS, --vers=VERS  Set version for ifdh_art setup
#    -c CONFIG, --config=CONFIG
#                          Specify config file (fcl) for art executable
#    -X EXE, --exe=EXE     Specify executable name for art executable -- default
#                          $EXPERIMENT
#    -g, --getconfig       get config files as inputs from SAM (i.e. for
#                          MonteCarlo simulation). Conflicts with --config
#    --dynamic_lifetime    for multifile and getconfig loops quit if less time
#                          than this left
#    --mix=MIX             mixing script to run after fetching each input
#    -M, --multifile       Fetch files in wrapper and run executable once per
#                          file
#    -G, --with_gdb        run executable under gdb and print stack trace
#    -L LIMIT, --limit=LIMIT
#                          Set SAM project file delivery limit
#    -S SCHEMA, --schema=SCHEMA
#                          Set SAM file delivery schema
#    --no-checksum         don't do checksums when adding metadata
#    -I INPUTFILE, --inputfile=INPUTFILE
#                          Input file to copy to job area before running the
#                          executable
#    -T INPUTTAR, --inputtar=INPUTTAR
#                          Input tar file to copy to job area and unpack running
#                          the executable
#    --inputlist=INPUTLIST
#                          ifdh cp -f file to run to fetch inputs
#    --cvmfs-revision=CVMFS_REVISION
#                          check for this cvmfs revision on
#                          /cvmfs/$GROUP.opensciencegrid.org/
#    --export=EXPORT       export environment variable before running
#    --export-unquote=EXPORT_UNQUOTE
#                          export environment variable before running
#    --setup=SETUP         setup product before running
#    --setup-unquote=SETUP_UNQUOTE
#                          setup product before running
#    --spack-load=SPACK_LOAD
#                          setup product before running
#    --spack-load-unquote=SPACK_LOAD_UNQUOTE
#                          setup product before running
#    --spack-env-activate=SPACK_ENV_ACTIVATE
#                          setup product before running
#    --spack-env-activate-unquote=SPACK_ENV_ACTIVATE_UNQUOTE
#                          setup product before running
#    --setup-local         setup all ups products in $INPUT_TAR_FILE directory
#    --setup_local         setup all ups products in $INPUT_TAR_FILE directory
#    --source=SOURCE       source setup file before running
#    --source-unquote=SOURCE_UNQUOTE
#                          source setup file before running
#    --self_destruct_timer=SELF_DESTRUCT_TIMER
#                          After this many seconds, suicide the job so we get
#                          output back
#    --self-destruct-timer=SELF_DESTRUCT_TIMER
#                          After this many seconds, suicide the job so we get
#                          output back
#    --prescript=PRESCRIPT
#                          script to run before Art executable
#    --prescript-unquote=PRESCRIPT_UNQUOTE
#                          script to run before Art executable
#    --finally=FINALLY     script to run before Art executable
#    --finally-unquote=FINALLY_UNQUOTE
#                          script to run before Art executable
#    --postscript=POSTSCRIPT
#                          script to run after Art executable
#    --postscript-unquote=POSTSCRIPT_UNQUOTE
#                          script to run after Art executable
#      --debug               Turn on debugging
#    --ifdh_art            executable can run the ifdh_art getNextFile loop
#    --data_dispatcher     Use Data Dispatcher and interface class to create
#                          files overwhich the executable will run
#    --data_dispatcher_task_id=DATA_DISPATCHER_TASK_ID
#                          Data Dispatcher Task ID (POMS)
#    --data_dispatcher_project=DATA_DISPATCHER_PROJECT
#                          Data Dispatcher Project ID
#    --data_dispatcher_dataset_query=DATA_DISPATCHER_DATASET_QUERY
#                          Data Dispatcher Dataset Query
#    --data_dispatcher_namespace=DATA_DISPATCHER_NAMESPACE
#                          Data Dispatcher Project namespace
#    --data_dispatcher_parameter=DATA_DISPATCHER_PARAMETER
#                          Data Dispatcher Custom Parameter
#    --data_dispatcher_load_limit=DATA_DISPATCHER_LOAD_LIMIT
#                          Data Dispatcher Load File Limit
#    --data_dispatcher_timeout=DATA_DISPATCHER_TIMEOUT
#                          Data Dispatcher Timeout (seconds)
#    --data_dispatcher_wait_time=DATA_DISPATCHER_WAIT_TIME
#                          Data Dispatcher Time to Wait Between Load Attempts
#                          (seconds)
#    --data_dispatcher_wait_limit=DATA_DISPATCHER_WAIT_LIMIT
#                          Data Dispatcher Max Number of Timeouts allowed
#    --data_dispatcher_user=DATA_DISPATCHER_USER
#                          Data Dispatcher User
#    --appname=APPNAME     application name for SAM
#    --appfamily=APPFAMILY
#                          application family for SAM
#    --appvers=APPVERS     application version for SAM
#    --userscript=USERSCRIPT
#                          extra user script to run after main executable
#    --find_setups         look in the 'usual places' for the ups setups script
#                          at startup
#    --start_project_on=START_PROJECT_ON
#                          start a sam project on this dataset
#    --end_project         look in the 'usual places' for the ups setups script
#                          at startup
#    --dry_run             Don't run commands, just print them
#    --nosetup             do not run setup actions (used internally)
#    --no_delete_after_copy
#                          set to not delete files after copying them out
#    --exe_stdout0=EXE_STDOUT0
#                          Specify output redirect for executable
#    --exe_stderr0=EXE_STDERR0
#                          Specify error output redirect for executable
#    [...]
#    --dest=DEST           Specify destination for copyBackOutput
#    --dest_uniq_rename    Copy to unique-ified dest dir, rename to dest
#                          directory afterwards
#    --rename=RENAME       Specify output file rename after Art runs
#    --addoutput=ADDOUTPUT
#                          glob pattern to match and call addOutputFile on
#    --declare_metadata    use given program to extract and declare metadata
#    --add_metadata=ADD_METADATA
#                          single metadata field key=value to add when declaring
#                          output files
#    --metadata_extractor=METADATA_EXTRACTOR
#                          use given program to extract and declare metadata
#    --metadata_extractor_unquote=METADATA_EXTRACTOR_UNQUOTE
#                          use given program to extract and declare metadata
#    --add_to_dataset=ADD_TO_DATASET
#                          Add files to named dataset using Dataset.Tag
#    --filter_metadata=FILTER_METADATA
#                          fields to filter out from metadata extractor, comma
#                          separated
#    --dataset_exclude=DATASET_EXCLUDE
#                          glob pattern of output files to exclude from the
#                          --add_to_dataset=dataset
#    --add_location        Add locations of output files to SAM
#    --outputlist=OUTPUTLIST
#                          ifdh cp -f file to run to copy out outputs
#    --hash=HASH           hash depth argument to copyBackOutput
#    --hash_alg=HASH_ALG   hash algorithm value for IFDH_DIR_HASH_ALG for
#                          copyBackOutput
#    --parallel=PARALLEL   hash algorithm value for IFDH_DIR_HASH_ALG for
#                          copyBackOutput
#    --dest1=DEST1         Specify destination for copyBackOutput
#    --dest_uniq_rename1   Copy to unique-ified dest dir, rename to dest
#                          directory afterwards
#    --rename1=RENAME1     Specify output file rename after Art runs
#    --addoutput1=ADDOUTPUT1
#                          glob pattern to match and call addOutputFile on
#    --declare_metadata1   use given program to extract and declare metadata
#    --add_metadata1=ADD_METADATA1
#                          single metadata field key=value to add when declaring
#                          output files
#    --metadata_extractor1=METADATA_EXTRACTOR1
#                          use given program to extract and declare metadata
#    --metadata_extractor_unquote1=METADATA_EXTRACTOR_UNQUOTE1
#                          use given program to extract and declare metadata
#    --add_to_dataset1=ADD_TO_DATASET1
#                          Add files to named dataset using Dataset.Tag
#    --filter_metadata1=FILTER_METADATA1
#                          fields to filter out from metadata extractor, comma
#                          separated
#    --dataset_exclude1=DATASET_EXCLUDE1
#                          glob pattern of output files to exclude from the
#                          --add_to_dataset=dataset
#    --add_location1       Add locations of output files to SAM
#    --outputlist1=OUTPUTLIST1
#                          ifdh cp -f file to run to copy out outputs
#    --hash1=HASH1         hash depth argument to copyBackOutput
#    --hash_alg1=HASH_ALG1
#                          hash algorithm value for IFDH_DIR_HASH_ALG for
#                          copyBackOutput
#    --parallel1=PARALLEL1
#                          hash algorithm value for IFDH_DIR_HASH_ALG for
#                          copyBackOutput
#  [...]
#
# 


#####################################################################################################
#####################################################################################################
[global]
#
# this section has variables we use later in the file as %(name)s
#
# Group/Experiment/Subgroup for priorities and accounting
group      = icarus
experiment = icarus

# fife wrapper. Its main tasks are the setup of the environment
# and the launch of the executables
wrapper    = file:///${FIFE_UTILS_DIR}/libexec/fife_wrap

# software specifications: version and qualifier
version    = override_me_version
quals      = override_me_qualifier

# basename of the produced files
basename = override_me_basename

# samweb dataset whose files have to be processed;
# usually overridden on command line with poms
sam_dataset = override_me_sam_dataset

# generic fcl file names
fclfile0 = override_me_0
fclfile1 = override_me_1
fclfile2 = override_me_2
fclfile3 = override_me_3
fclfile4 = override_me_4

# wrapper from fcl files used to have additional
# metadata into the files
wrapperfclfile0 = wrapper_%(fclfile0)s
wrapperfclfile1 = wrapper_%(fclfile1)s
wrapperfclfile2 = wrapper_%(fclfile2)s
wrapperfclfile3 = wrapper_%(fclfile3)s
wrapperfclfile4 = wrapper_%(fclfile4)s

# larsoft file processing stages to run
stagename0 = gen
stagename1 = g4
stagename2 = stage0
stagename3 = stage1
stagename4 = caf

# prodtype can be production/test
prodtype = override_me_prodtype

# filetype from samweb
# it can be mc/data/test_data/unknown
filetype = mc

# name of the campaign to which the sample belongs
campaign = override_me_campaign_name

# name of the sample
sample   = override_me_sample_name

# Destination Folder Structure
sbndatapool         = data
dataset_name_struct = %(prodtype)s_%(filetype)s_%(campaign)s_%(sample)s_%(version)s
dest_struct         = %(filetype)s/%(campaign)s/%(sample)s/%(version)s
data_dest           = /pnfs/sbn/%(sbndatapool)s/sbn_fd/poms_%(prodtype)s/%(dest_struct)s
scratch_dest        = /pnfs/%(experiment)s/scratch/users/%(experiment)spro/poms_%(prodtype)s/%(dest_struct)s

# number of events per job
nevt     = -1

# run, subrun, event
firstrun     = 1
first_subrun = \${PROCESS}
first_enum   = \$\(\(PROCESS*%(nevt)s+1\)\)
first_event  = %(firstrun)s:%(first_subrun)s:%(first_enum)s
fclfilename0 = override_me_fclfilename0

# stage_name
stage_name = override_me_stage_name

# streamname
streamname = override_me_streamname

#####################################################################################################
#####################################################################################################
[env_pass]
#
# these become -e parameters to jobsub_submit
# 
#      -e ENV_VAR, --environment=ENV_VAR
#                          -e  ADDED_ENVIRONMENT exports this variable with its
#                          local  value to worker node environment. For example
#                          export FOO="BAR";  jobsub -e FOO <more stuff>
#                          guarantees that the value of $FOO on  the worker node
#                          is "BAR" .  Alternate format which does not require
#                          setting the env var first is the -e VAR=VAL, idiom
#                          which  sets the value of $VAR to 'VAL' in the worker
#                          environment. The  -e  option can be used as many times
#                          in one jobsub_submit  invocation as desired
#
# Here the list of the Jobsub Environment Variables: 
# https://cdcvs.fnal.gov/redmine/projects/jobsub/wiki/Jobsub_enviornment_variables

# IFDH
IFDH_DEBUG        = 1
IFDH_BASE_URI     = http://samsbn.fnal.gov:8480/sam/sbn/api/
IFDH_TOKEN_ENABLE = 1
IFDH_PROXY_ENABLE = 0

# SAM
SAM_EXPERIMENT    = sbn
SAM_GROUP         = sbn
SAM_STATION       = sbn
SAM_WEB_HOST      = samsbn.fnal.gov

LC_ALL            = C

#####################################################################################################
#####################################################################################################
[submit]
#
# these become options to jobsub_submit
#
#    -G <Group/Experiment/Subgroup>, --group=<Group/Experiment/Subgroup>
#                        Group/Experiment/Subgroup for priorities and
#                        accounting
G                          = %(group)s

#      -N NUM              submit N copies  of this job. Each job will  have
#                          access to the environment variable  $PROCESS that
#                          provides the job number (0 to  NUM-1), equivalent to
#                          the number following the decimal point in  the job ID
#                          (the '2' in 134567.2).
N                          = 5

#      --dataset_definition=DATASET_DEFINITION
#                          SAM dataset definition used in a Directed Acyclic
#                          Graph (DAG)
dataset                    =

#      --resource-provides=RESOURCE_PROVIDES
#                          request specific  resources by changing condor jdf
#                          file.  For example: --resource-provides=CVMFS=OSG
#                          will add +CVMFS="OSG" to the job classad  attributes
#                          and '&&(CVMFS=="OSG")' to the  job requirements
resource-provides          = usage_model=OPPORTUNISTIC,DEDICATED,OFFSITE

#      --generate-email-summary
#                          generate and mail a summary report of
#                          completed/failed/removed  jobs in a DAG
generate-email-summary     = True

#      --expected-lifetime='short'|'medium'|'long'|NUMBER[UNITS]
#                          Expected lifetime of the job.  Used to match against
#                          resources advertising that they have
#                          REMAINING_LIFETIME seconds left.  The shorter your
#                          EXPECTED_LIFTIME is, the more resources (aka slots,
#                          cpus) your job can potentially match against and the
#                          quicker it should start.  If your job runs longer than
#                          EXPECTED_LIFETIME it *may* be killed by the batch
#                          system.  If your specified  EXPECTED_LIFETIME is too
#                          long your job may take a long time to match against  a
#                          resource a sufficiently long REMAINING_LIFETIME.
#                          Valid inputs for this parameter are 'short', 'medium',
#                          'long', or NUMBER[UNITS] of time.  IF [UNITS] is
#                          omitted, value is NUMBER  seconds. Allowed values for
#                          UNITS are 's', 'm', 'h', 'd' representing seconds,
#                          minutes, etc.The values for 'short','medium',and
#                          'long' are configurable by Grid Operations, they
#                          currently are '3h' , '8h' , and '85200s' but this may
#                          change in the future. Default value of
#                          EXPECTED_LIFETIME is currently '8h'.
expected-lifetime          = 4h

#      --timeout=NUMBER[UNITS]
#                          kill user job if still running after NUMBER[UNITS] of
#                          time . UNITS may be `s' for seconds (the default), `m'
#                          for minutes, `h' for hours or `d' h for days.
timeout                    = 3h

#      --disk=NUMBER[UNITS]
#                          Request worker nodes have at least NUMBER[UNITS] of
#                          disk space.    If UNITS is not specified default is
#                          'KB' (a typo in earlier versions  said that default
#                          was 'MB', this was wrong).  Allowed values for  UNITS
#                          are 'KB','MB','GB', and 'TB'
disk                       = 50GB

#      --memory=NUMBER[UNITS]
#                          Request worker nodes have at least NUMBER[UNITS]  of
#                          memory.  If UNITS is not specified default is 'MB'.
#                          Allowed values for  UNITS are 'KB','MB','GB', and 'TB'
memory                     = 2000MB

#      -f INPUT_FILE       at runtime, INPUT_FILE will be copied to directory
#                          $CONDOR_DIR_INPUT on the execution node.  Example :-f
#                          /grid/data/minerva/my/input/file.xxx  will be copied
#                          to $CONDOR_DIR_INPUT/file.xxx  Specify as many -f
#                          INPUT_FILE_1 -f INPUT_FILE_2  args as you need.  To
#                          copy file at submission time  instead of run time, use
#                          -f dropbox://INPUT_FILE to  copy the file.
f_0                        = /pnfs/icarus/resilient/icaruspro/poms_scripts/icaruspoms_tfilemetadata_extractor.sh

#      -c APPEND_REQUIREMENTS, --append_condor_requirements=APPEND_REQUIREMENTS
#                          append condor requirements
append_condor_requirements = '((TARGET.has_avx==true)&&(TARGET.HAS_CVMFS_icarus_opensciencegrid_org==true))&&(TARGET.HAS_SINGULARITY=?=true)'

#      -l "line", --lines="line" 
#                          [Expert option]  Add  "line" to the Condor  submission
#                          (.cmd) file, typically as a classad attribute.   See
#                          the HTCondor documentation  for more.
lines_1                    = '+SingularityImage=\"/cvmfs/singularity.opensciencegrid.org/fermilab/fnal-wn-sl7:latest\"'
lines_2                    = '+FERMIHTC_AutoRelease=True'
lines_3                    = '+FERMIHTC_GraceMemory=3000'
lines_4                    = '+FERMIHTC_GraceLifetime=7200'

#      --subgroup=SUBGROUP
#                          Subgroup for priorities and accounting. See
#                          https://cdcvs.fnal.gov/redmine/projects/jobsub/wiki/
#                          Jobsub_submit#Groups-Subgroups-Quotas-Priorities  for
#                          more documentation on using --subgroup to set job
#                          quotas and priorities
subgroup                   = pro

#      --blacklist=COMMA,SEP,LIST,OF,SITES
#                          ensure that jobs do not land at these sites
blacklist                  = RAL

#####################################################################################################
#####################################################################################################
[job_setup]
#
# these are options to fife_wrap about setting up the job environment,
# and main execution loop
#
#   --debug              Turn on debugging
debug        = True

#  --find_setups         look in the 'usual places' for the ups setups script
#                        at startup
find_setups  = True

#  --setup_local         setup all ups products in $INPUT_TAR_FILE directory
setup_local  = True

#  --source=SOURCE       source setup file before running
source_0     = /cvmfs/%(experiment)s.opensciencegrid.org/products/%(experiment)s/setup_%(experiment)s.sh

#  --setup=SETUP         setup product before running
setup_0      = %(experiment)scode %(version)s -q %(quals)s
setup_1      = sbnutil -q sbn

#  --prescript=PRESCRIPT
#                        script to run before Art executable
prescript_0  = export FW_SEARCH_PATH=$FW_SEARCH_PATH:$CONDOR_DIR_INPUT
prescript_1  = export FHICL_FILE_PATH=$FHICL_FILE_PATH:$CONDOR_DIR_INPUT

#  -M, --multifile       Fetch files in wrapper and run executable once per
#                        file
multifile    = False

#####################################################################################################
#####################################################################################################
[sam_consumer]
#
# Specify options for the establishConsumer call if using 
# ifdh_art/multifile/getconfig options above
#
#    - limit = n         -- limit number of files to send this consumer
#    - schema = x        -- file delivery schema(s) preferred {root (streaming),gsiftp,..}
#    - appname = name    -- application name to associate with consumer
#    - appfamily = name  -- application family "
#    - appvers = version -- application version "
limit       = 1
appvers     = %(version)s
appfamily   = art
appname     = %(stage_name)s
schema      = root

#####################################################################################################
#####################################################################################################
[executable]
#
# Primary command your job is going to run. Allows you to specify further 
# executables to run -- i.e. if you want to run a simulation and reco in the same job
#
#    - name = executable-name  -- primary executable to run
#    - arg_1 = first-argument  -- first argument
#    - arg_2 = second-argument -- second argument
#    - exe_stdout = yyy        -- file to redirect standard output
#    - exe_stderr = yyy        -- file to redirect standard error
#
# N.B. Here we set the maximum number of executables that could be used in the same job
#      The trick to enable only the ones that are needed is to set the name for the 
#      corresponding executable to the actual executable, while setting the name to 
#      true for those to be disabled (true anything simply returns exit code 0 without 
#      doing anything). In this way, one can have N executable sections and activate only 
#      M of them, with M <= N."
name       = true
arg_1      = -c
arg_2      = \$CONDOR_DIR_INPUT/%(wrapperfclfile0)s
arg_3      = -o
arg_4      = %%ifb_%%tc_%(basename)s.root
arg_5      = -T 
arg_6      = hist_%%ifb_%%tc_%(basename)s.root
arg_7      = -n 
arg_8      = %(nevt)s 
arg_9      = --sam-data-tier
arg_10     = simulation
arg_11     = --sam-stream-name
arg_12     = out1
arg_13     = -s
arg_14     = 

#####################################################################################################
#####################################################################################################
[executable_1]
#
#  Primary command your job is going to run. Allows you to specify further 
#  executables to run -- i.e. if you want to run a simulation and reco in the same job
#
name       = true
arg_1      = -c
arg_2      = %(fclfile1)s
arg_3      = -o
arg_4      = %(basename)s_\${CLUSTER}_\${PROCESS}.root
arg_5      = -T 
arg_6      = hist_%(basename)s_\${CLUSTER}_\${PROCESS}.root
arg_7      = -n 
arg_8      = %(nevt)s 
arg_9      = --sam-data-tier
arg_10     = simulation
arg_11     = --sam-stream-name
arg_12     = out1
arg_13     = -s
arg_14     = input_filename -- will be added by multifile loop...

#####################################################################################################
#####################################################################################################
[executable_2]
#
#  Primary command your job is going to run. Allows you to specify further 
#  executables to run -- i.e. if you want to run a simulation and reco in the same job
#
name       = true
arg_1      = -c
arg_2      = %(fclfile2)s
arg_3      = -o
arg_4      = %(basename)s_\${CLUSTER}_\${PROCESS}.root
arg_5      = -T 
arg_6      = hist_%(basename)s_\${CLUSTER}_\${PROCESS}.root
arg_7      = -n 
arg_8      = %(nevt)s 
arg_9      = --sam-data-tier
arg_10     = simulation
arg_11     = --sam-stream-name
arg_12     = out1
arg_13     = -s
arg_14     = input_filename -- will be added by multifile loop...

#####################################################################################################
#####################################################################################################
[executable_3]
#
#  Primary command your job is going to run. Allows you to specify further 
#  executables to run -- i.e. if you want to run a simulation and reco in the same job
#
name       = true
arg_1      = -c
arg_2      = %(fclfile3)s
arg_3      = -o
arg_4      = %(basename)s_\${CLUSTER}_\${PROCESS}.root
arg_5      = -T 
arg_6      = hist_%(basename)s_\${CLUSTER}_\${PROCESS}.root
arg_7      = -n 
arg_8      = %(nevt)s 
arg_9      = --sam-data-tier
arg_10     = simulation
arg_11     = --sam-stream-name
arg_12     = out1
arg_13     = -s
arg_14     = input_filename -- will be added by multifile loop...

#####################################################################################################
#####################################################################################################
[executable_4]
#
#  Primary command your job is going to run. Allows you to specify further 
#  executables to run -- i.e. if you want to run a simulation and reco in the same job
#
name       = true
arg_1      = -c
arg_2      = %(fclfile4)s
arg_3      = -o
arg_4      = %(basename)s_\${CLUSTER}_\${PROCESS}.root
arg_5      = -T 
arg_6      = hist_%(basename)s_\${CLUSTER}_\${PROCESS}.root
arg_7      = -n 
arg_8      = %(nevt)s 
arg_9      = --sam-data-tier
arg_10     = simulation
arg_11     = --sam-stream-name
arg_12     = out1
arg_13     = -s
arg_14     = input_filename -- will be added by multifile loop...

#####################################################################################################
#####################################################################################################
[executable_5]
#
#  Primary command your job is going to run. Allows you to specify further 
#  executables to run -- i.e. if you want to run a simulation and reco in the same job
#
name       = true
arg_1      = -c
arg_2      = %(fclfile4)s
arg_3      = -o
arg_4      = %(basename)s_\${CLUSTER}_\${PROCESS}.root
arg_5      = -T 
arg_6      = hist_%(basename)s_\${CLUSTER}_\${PROCESS}.root
arg_7      = -n 
arg_8      = %(nevt)s 
arg_9      = --sam-data-tier
arg_10     = simulation
arg_11     = --sam-stream-name
arg_12     = out1
arg_13     = -s
arg_14     = input_filename -- will be added by multifile loop...

#####################################################################################################
#####################################################################################################
[job_output]
#
# parameters to output handling section of fife_wrap
#
#  --rename=RENAME       Specify output file rename after Art runs
rename       = unique

#  --addoutput=ADDOUTPUT
#                        glob pattern to match and call addOutputFile on
addoutput    = *.[ol][ou][gt]

#  --add_location        Add locations of output files to SAM
add_location = False

#  --dest=DEST           Specify destination for copyBackOutput
dest         = /pnfs/%(experiment)s/scratch/users/%(experiment)spro/dropbox/mc1/dropbox/mc1/logs/

#  --hash=HASH           hash depth argument to copyBackOutput
hash 	     = 2

#####################################################################################################
#####################################################################################################
[job_output_1]
#
# parameters to output handling section of fife_wrap
#
rename       = unique
addoutput    = *.[ol][ou][gt]
add_location = False       
dest         = /pnfs/%(experiment)s/scratch/users/%(experiment)spro/dropbox/mc1/dropbox/mc1/logs/
hash 	     = 2

#####################################################################################################
#####################################################################################################
[job_output_2]
#
# parameters to output handling section of fife_wrap
#
addoutput          = hist*%(basename)s*.root
rename             = unique
dest               = /pnfs/%(experiment)s/scratch/users/%(experiment)spro/dropbox/mc1/dropbox/mc1/hist

#  --declare_metadata1   use given program to extract and declare metadata
declare_metadata   = True

#  --metadata_extractor1=METADATA_EXTRACTOR1
#                        use given program to extract and declare metadata
metadata_extractor = sam_metadata_dumper

hash 	           = 2

#####################################################################################################
#####################################################################################################
[job_output_3]
#
# parameters to output handling section of fife_wrap
#
addoutput          = Suppl*.root
dest               = /pnfs/%(experiment)s/scratch/users/%(experiment)spro/dropbox/mc1/dropbox/mc1/calib
metadata_extractor = sam_metadata_dumper
add_location       = True
hash               = 2

#####################################################################################################
#####################################################################################################
[job_output_4]
#
# parameters to output handling section of fife_wrap
#
addoutput          = Suppl*.root
dest               = /pnfs/%(experiment)s/scratch/users/%(experiment)spro/dropbox/mc1/dropbox/mc1/calib
add_location       = True
metadata_extractor = sam_metadata_dumper
hash               = 2

#####################################################################################################
#####################################################################################################
[job_output_5]
#
# parameters to output handling section of fife_wrap
#
addoutput          = Suppl*.root
dest               = /pnfs/%(experiment)s/scratch/users/%(experiment)spro/dropbox/mc1/dropbox/mc1/calib
add_location       = True
metadata_extractor = sam_metadata_dumper
hash               = 2


#
# now we have overides for each processing stage/job type 
#
# you may need to change the global.fclfile overrides for each
# stage to reflect your experiment's naming convention for .fcl files
#

#####################################################################################################
#####################################################################################################
[stage_gen_g4]
# in this stage we will run gen (executable) and g4 (executable_1) stages
# and we will copy back to scratch location only g4 files

# global
global.fcl_list   = %(fclfile0)s/%(fclfile1)s
global.basename   = %(fclfilename0)s
global.stage_name = %(stagename0)s_%(stagename1)s

# env_pass
env_pass.IFDH_FORCE         = https
env_pass.IFDH_CP_MAXRETRIES = 5
env_pass.IFDH_HTTPS_EXTRA   = -f

# submit
submit.append_condor_requirements = '(CVMFS_sbn_osgstorage_org_REVISION>=142423)'

# job_setup
job_setup.ifdh_art  = False
job_setup.multifile = False

# job_setup : prescripts
job_setup.prescript_2  = sh sbnpoms_wrapperfcl_maker.sh --fclname %(fclfile0)s --wrappername $CONDOR_DIR_INPUT/%(wrapperfclfile0)s
job_setup.prescript_3  = sh sbnpoms_wrapperfcl_maker.sh --fclname %(fclfile1)s --wrappername $CONDOR_DIR_INPUT/%(wrapperfclfile1)s
job_setup.prescript_4  = cat $CONDOR_DIR_INPUT/%(wrapperfclfile0)s
job_setup.prescript_5  = cat $CONDOR_DIR_INPUT/%(wrapperfclfile1)s
job_setup.prescript_6  = sh sbnpoms_runnumber_injector.sh --fcl $CONDOR_DIR_INPUT/%(wrapperfclfile0)s --run %(firstrun)s --subruns_per_run 10 --process ${JOBSUBJOBSECTION}
job_setup.prescript_7  = sh sbnpoms_flux_injector.sh --fcl $CONDOR_DIR_INPUT/%(wrapperfclfile0)s
job_setup.prescript_8  = sh sbnpoms_metadata_injector.sh --inputfclname $CONDOR_DIR_INPUT/%(wrapperfclfile0)s --mdfclname %(fclfile0)s --mdprojectname %(campaign)s --mdprojectstage  %(stage_name)s --mdprojectversion %(version)s --mdprojectsoftware %(experiment)scode --mdproductionname %(sample)s --mdproductiontype %(prodtype)s --mdappversion %(version)s --mdfiletype %(filetype)s --mdappfamily art --mdruntype physics --mdgroupname %(experiment)s
job_setup.prescript_9  = sh sbnpoms_metadata_injector.sh --inputfclname $CONDOR_DIR_INPUT/%(wrapperfclfile1)s --mdfclname %(fcl_list)s --mdprojectname %(campaign)s --mdprojectstage %(stage_name)s --mdprojectversion %(version)s --mdprojectsoftware %(experiment)scode --mdproductionname %(sample)s --mdproductiontype %(prodtype)s --mdappversion %(version)s --mdfiletype %(filetype)s --mdappfamily art --mdruntype physics --mdgroupname %(experiment)s
job_setup.prescript_10 = echo "This PROCESS is ${PROCESS}"
job_setup.prescript_11 = echo "This JOBSUBJOBSECTION is ${JOBSUBJOBSECTION}"

# job_setup : postcripts
job_setup.postscript   = ls -ltrh
job_setup.postscript_1 = echo "=========================="
job_setup.postscript_2 = cat $CONDOR_DIR_INPUT/%(wrapperfclfile0)s
job_setup.postscript_3 = echo "=========================="
job_setup.postscript_4 = cat %(fclfile0)s
job_setup.postscript_5 = echo "=========================="
job_setup.postscript_6 = cat $CONDOR_DIR_INPUT/%(wrapperfclfile1)s
job_setup.postscript_7 = du -h
job_setup.postscript_8 = echo "This PROCESS is ${PROCESS}"
job_setup.postscript_9 = echo "This JOBSUBJOBSECTION is ${JOBSUBJOBSECTION}"

# sam_consumer
sam_consumer.limit = 1

# executable: executable
# i.e. gen
executable.name   = lar
executable.arg_1  = -c
executable.arg_2  = \$CONDOR_DIR_INPUT/%(wrapperfclfile0)s
executable.arg_3  = -o
executable.arg_4  = %(basename)s_%(stagename0)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.root
executable.arg_5  = -T
executable.arg_6  = hist_%(basename)s_%(stagename0)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.root
executable.arg_7  = -n
executable.arg_8  = %(nevt)s
executable.arg_9  = --sam-data-tier
executable.arg_10 = %(stagename0)s
executable.arg_11 = --sam-stream-name
executable.arg_12 = %(streamname)s
executable.arg_13 = 
executable.arg_14 = 

# executable: executable_1
# i.e. g4
executable_1.name   = lar
executable_1.arg_1  = -c
executable_1.arg_2  = \$CONDOR_DIR_INPUT/%(wrapperfclfile1)s
executable_1.arg_3  = -s
executable_1.arg_4  = %(basename)s_%(stagename0)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.root
executable_1.arg_5  = -o
executable_1.arg_6  = %(basename)s_%(stagename1)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.root
executable_1.arg_7  = -T
executable_1.arg_8  = hist_%(basename)s_%(stagename1)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.root
executable_1.arg_9  = -n
executable_1.arg_10 = %(nevt)s
executable_1.arg_11 = --sam-data-tier
executable_1.arg_12 = %(stagename1)s
executable_1.arg_13 = --sam-stream-name
executable_1.arg_14 = %(streamname)s

# job_output: job_output
job_output.declare_metadata   = True
job_output.metadata_extractor = sam_metadata_dumper
job_output.add_location       = True       
job_output.hash               = 2

#  --add_to_dataset=ADD_TO_DATASET
#                        Add files to named dataset using Dataset.Tag
job_output.add_to_dataset     = %(dataset_name_struct)s_%(stagename1)s

#  --dataset_exclude=DATASET_EXCLUDE
#                        glob pattern of output files to exclude from the
#                        --add_to_dataset=dataset
job_output.dataset_exclude    = hist*

#  --rename=RENAME       Specify output file rename after Art runs
job_output.rename             = unique

#  --filter_metadata=FILTER_METADATA
#                        fields to filter out from metadata extractor, comma
#                        separated
job_output.filter_metadata_1  = parents

job_output.addoutput          = %(basename)s_%(stagename1)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.root
job_output.dest               = %(scratch_dest)s/%(stagename1)s/

#####################################################################################################
#####################################################################################################
[stage_detsim_stage0_stage1_caf]
# in this stage we will run:
#  - detsim (executable)  . It produces: detsim and hist_detsim files
#  - stage0 (executable_1). It produces: stage0 and hist_detsim files
#  - stage1 (executable_2). It produces: stage1 and calib_ntuples files
#  - caf    (executable_3). It produces: caf and caf.flat files
# then we will copy back to scratch location:
#  - stage0 files
#  - stage1 files
#  - calib_ntuples files
# and we will copy back to disk (data):
#  - caf files
#  - flat.caf files

# global
global.basename   = %(fclfilename0)s
global.stage_name = %(stagename0)s_%(stagename1)s_%(stagename2)s_%(stagename3)s
global.fcl_list   = %(fclfile3)s/%(fclfile2)s/%(fclfile1)s/%(fclfile0)s

# submit
submit.cpu     = 2
submit.dataset = %(sam_dataset)s

# job_setup
job_setup.setup_2   = jemalloc v5_3_0b -q %(quals)s
job_setup.ifdh_art  = False
job_setup.multifile = True

# job_setup: prescript
job_setup.prescript_2  = sh sbnpoms_wrapperfcl_maker.sh --fclname %(fclfile0)s --wrappername $CONDOR_DIR_INPUT/%(wrapperfclfile0)s
job_setup.prescript_3  = sh sbnpoms_wrapperfcl_maker.sh --fclname %(fclfile1)s --wrappername $CONDOR_DIR_INPUT/%(wrapperfclfile1)s
job_setup.prescript_4  = sh sbnpoms_wrapperfcl_maker.sh --fclname %(fclfile2)s --wrappername $CONDOR_DIR_INPUT/%(wrapperfclfile2)s
job_setup.prescript_5  = sh sbnpoms_wrapperfcl_maker.sh --fclname %(fclfile3)s --wrappername $CONDOR_DIR_INPUT/%(wrapperfclfile3)s
job_setup.prescript_6  = sbnpoms_metadata_injector.sh --inputfclname $CONDOR_DIR_INPUT/%(wrapperfclfile0)s --mdfclname %(fcl_list)s --mdprojectname %(campaign)s --mdprojectstage  %(stage_name)s --mdprojectversion %(version)s --mdprojectsoftware %(experiment)scode --mdproductionname  %(sample)s --mdproductiontype %(prodtype)s --mdappversion %(version)s --mdfiletype %(filetype)s --mdappfamily art --mdruntype physics --mdprojectversion %(version)s --mdgroupname %(experiment)s
job_setup.prescript_7  = sbnpoms_metadata_injector.sh --inputfclname $CONDOR_DIR_INPUT/%(wrapperfclfile1)s --mdfclname %(fcl_list)s --mdprojectname %(campaign)s --mdprojectstage  %(stage_name)s --mdprojectversion %(version)s --mdprojectsoftware %(experiment)scode --mdproductionname  %(sample)s --mdproductiontype %(prodtype)s --mdappversion %(version)s --mdfiletype %(filetype)s --mdappfamily art --mdruntype physics --mdprojectversion %(version)s --mdgroupname %(experiment)s
job_setup.prescript_8  = sbnpoms_metadata_injector.sh --inputfclname $CONDOR_DIR_INPUT/%(wrapperfclfile2)s --mdfclname %(fcl_list)s --mdprojectname %(campaign)s --mdprojectstage  %(stage_name)s --mdprojectversion %(version)s --mdprojectsoftware %(experiment)scode --mdproductionname  %(sample)s --mdproductiontype %(prodtype)s --mdappversion %(version)s --mdfiletype %(filetype)s --mdappfamily art --mdruntype physics --mdprojectversion %(version)s --mdgroupname %(experiment)s
job_setup.prescript_9  = sbnpoms_metadata_injector.sh --inputfclname $CONDOR_DIR_INPUT/%(wrapperfclfile3)s --mdfclname %(fcl_list)s --mdprojectname %(campaign)s --mdprojectstage  %(stage_name)s --mdprojectversion %(version)s --mdprojectsoftware %(experiment)scode --mdproductionname  %(sample)s --mdproductiontype %(prodtype)s --mdappversion %(version)s --mdfiletype %(filetype)s --mdappfamily art --mdruntype physics --mdprojectversion %(version)s --mdgroupname %(experiment)s
job_setup.prescript_10 = echo "This PROCESS is ${PROCESS}"
job_setup.prescript_11 = echo "This JOBSUBJOBSECTION is ${JOBSUBJOBSECTION}"

# job_setup: postcripts
job_setup.postscript    = echo "=========================="
job_setup.postscript_1  = cat $CONDOR_DIR_INPUT/%(wrapperfclfile0)s
job_setup.postscript_2  = echo "=========================="
job_setup.postscript_3  = cat %(fclfile0)s
job_setup.postscript_4  = echo "=========================="
job_setup.postscript_5  = cat $CONDOR_DIR_INPUT/%(wrapperfclfile1)s
job_setup.postscript_6  = echo "=========================="
job_setup.postscript_7  = cat $CONDOR_DIR_INPUT/%(wrapperfclfile2)s
job_setup.postscript_8  = echo "=========================="
job_setup.postscript_9  = cat $CONDOR_DIR_INPUT/%(wrapperfclfile3)s
job_setup.postscript_10 = echo "=========================="
job_setup.postscript_11 = ls -ltrh
job_setup.postscript_12 = du -h
job_setup.postscript_13 = echo "This PROCESS is ${PROCESS}"
job_setup.postscript_14 = echo "This JOBSUBJOBSECTION is ${JOBSUBJOBSECTION}"

# sam_consumer
sam_consumer.limit = 1

# executable: executable
# i.e. detsim
executable.name   = lar
executable.arg_1  = -c
executable.arg_2  = \$CONDOR_DIR_INPUT/%(wrapperfclfile0)s
executable.arg_3  = -o
executable.arg_4  = %(basename)s_%(stagename0)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.root
executable.arg_5  = -T 
executable.arg_6  = hist_%(basename)s_%(stagename0)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.root
executable.arg_7  = -n
executable.arg_8  = %(nevt)s
executable.arg_9  = --sam-data-tier
executable.arg_10 = %(stagename0)s
executable.arg_11 = --sam-stream-name
executable.arg_12 = %(streamname)s
executable.arg_13 = -s
executable.arg_14 = 

# executable: executable_1
# i.e. stage0
executable_1.name   = LD_PRELOAD=\\\\\$JEMALLOC_LIB/libjemalloc.so
executable_1.arg_1  = lar
executable_1.arg_2  = -c
executable_1.arg_3  = \$CONDOR_DIR_INPUT/%(wrapperfclfile1)s
executable_1.arg_4  = -o
executable_1.arg_5  = %(basename)s_%(stagename1)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.root
executable_1.arg_6  = -T
executable_1.arg_7  = hist_%(basename)s_%(stagename1)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.root
executable_1.arg_8  = -n
executable_1.arg_9  = %(nevt)s
executable_1.arg_10 = --sam-data-tier
executable_1.arg_11 = %(stagename1)s
executable_1.arg_12 = --sam-stream-name
executable_1.arg_13 = %(streamname)s
executable_1.arg_14 = -s
executable_1.arg_15 = %(basename)s_%(stagename0)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.root

# MT?
#   -> YES
executable_1.arg_16 = --nschedules
executable_1.arg_17 = 1
executable_1.arg_18 = --nthreads
executable_1.arg_19 = 2

# MT?
#   -> NO
#executable_1.arg_16 = 
#executable_1.arg_17 = 
#executable_1.arg_18 = 
#executable_1.arg_19 = 

# executable: executable_2
# i.e. stage1
executable_2.name   = lar
executable_2.arg_1  = -c
executable_2.arg_2  = \$CONDOR_DIR_INPUT/%(wrapperfclfile2)s
executable_2.arg_3  = -o
executable_2.arg_4  = %(basename)s_%(stagename2)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.root
executable_2.arg_5  = -T
executable_2.arg_6  = calib_ntuples_%(basename)s_%(stagename2)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.root
executable_2.arg_7  = -n
executable_2.arg_8  = %(nevt)s
executable_2.arg_9  = --sam-data-tier
executable_2.arg_10 = simulation
executable_2.arg_11 = --sam-stream-name
executable_2.arg_12 = %(streamname)s
executable_2.arg_13 = -s
executable_2.arg_14 = %(basename)s_%(stagename1)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.root

# executable: executable_3
# i.e. CAF
executable_3.name   = lar
executable_3.arg_1  = -c
executable_3.arg_2  = \$CONDOR_DIR_INPUT/%(wrapperfclfile3)s
executable_3.arg_3  = -s
executable_3.arg_4  = %(basename)s_%(stagename2)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.root
executable_3.arg_5  = -n
executable_3.arg_6  = %(nevt)s
executable_3.arg_7  =
executable_3.arg_8  =
executable_3.arg_9  =
executable_3.arg_10 = 
executable_3.arg_11 = 
executable_3.arg_12 = 
executable_3.arg_13 = 
executable_3.arg_14 = 

# job_output: job_output
job_output.add_to_dataset     = %(dataset_name_struct)s_%(stagename2)s
job_output.metadata_extractor = sam_metadata_dumper
job_output.addoutput          = %(basename)s_%(stagename2)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.root
job_output.rename             = unique
job_output.filter_metadata_1  = parents
job_output.declare_metadata   = True
job_output.add_location       = True
job_output.hash               = 2
job_output.dest               = %(scratch_dest)s/%(stagename2)s/

# job_output: job_output_1
job_output_1.add_to_dataset     = %(dataset_name_struct)s_caf
job_output_1.metadata_extractor = sbnpoms_metadata_extractor.py
job_output_1.addoutput          = %(basename)s_%(stagename2)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.caf.root
job_output_1.rename             = unique
job_output_1.add_metadata_1     = file_format=caf
job_output_1.add_metadata_2     = file_type=mc
job_output_1.declare_metadata   = True
job_output_1.add_location       = True
job_output_1.hash               = 2
job_output_1.dest               = %(data_dest)s/caf/

# job_output: job_output_2
job_output_2.add_to_dataset     = %(dataset_name_struct)s_flatcaf
job_output_2.metadata_extractor = sbnpoms_metadata_extractor.py
job_output_2.addoutput          = %(basename)s_%(stagename2)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.flat.caf.root
job_output_2.rename             = unique
job_output_2.add_metadata_1     = file_format=flat_caf
job_output_2.add_metadata_2     = file_type=mc
job_output_2.declare_metadata   = True
job_output_2.add_location       = True
job_output_2.hash               = 2
job_output_2.dest               = %(data_dest)s/flatcaf/

# job_output: job_output_3
job_output_3.add_to_dataset     = %(dataset_name_struct)s_calibtuple
job_output_3.metadata_extractor = sbnpoms_metadata_extractor.py
job_output_3.addoutput          = calib_ntuples_%(basename)s_%(stagename2)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.root
job_output_3.rename             = unique
job_output_3.declare_metadata   = True
job_output_3.add_location       = True
job_output_3.add_metadata_1     = file_format=calib_ntuples
job_output_3.add_metadata_2     = file_type=mc
job_output_3.hash 	            = 2
job_output_3.dest               = %(scratch_dest)s/calibtuple/

# job_output: job_output_4
job_output_4.add_to_dataset     = %(dataset_name_struct)s_%(stagename1)s
job_output_4.metadata_extractor = sam_metadata_dumper
job_output_4.addoutput          = %(basename)s_%(stagename1)s_\${CLUSTER}_\${JOBSUBJOBSECTION}.root
job_output_4.rename             = unique
job_output_4.filter_metadata_1  = parents
job_output_4.declare_metadata   = True
job_output_4.add_location       = True
job_output_4.hash               = 2
job_output_4.dest               = %(scratch_dest)s/%(stagename1)s/
